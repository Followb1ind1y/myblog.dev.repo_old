---
title: "Regularization for Deep Learning"
date: "2021-07-25"
tags: ["L1 Regularization", "L2 Regularization", "Dataset Augmentation", "Noise Injection", "Early Stopping", "Dropout"]
categories: ["Data Science", "Deep Learning", "Regularization"]
weight: 3
---

## Overfitting, Underfitting and Capacity

The central challenge in machine learning is that we must perform well on new, previously unseen inputs—not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called **generalization**.

> 机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好, 而不只是在训练集上表现良好.在先前未观测到的输入上表现良好的能力被称为 **泛化(generalization)** .

Typically, when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the **training error**, and we reduce this training error. So far, what we have described is simply an optimization problem. What separates machine learning from optimization is that we want the **generalization error**, also called the **test error**, to be low as well. The generalization error is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice.

> 通常情况下, 当我们训练机器学习模型时, 我们可以使用某个训练集, 在训练集上计算一些被称为 **训练误差(training error)** 的度量误差, 目标是降低训练误差.机器学习和优化不同的地方在于, 我们也希望 **泛化误差(generalization error)** (也被称为 **测试误差(test error)** )很低.

We typically estimate the generalization error of a machine learning model by measuring its performance on a **test set** of examples that were collected separately from the training set.

The train and test data are generated by a probability distribution over datasets called the **data generating process**. We typically make a set of assumptions known collectively as the **i.i.d. assumptions**. These assumptions are that the examples in each dataset are **independent** from each other, and that the train set and test set are **identically distributed**, drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the **data generating distribution**, denoted `$p_{data}$`. This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error.

One immediate connection we can observe between the training and test error is that the expected training error of a randomly selected model is equal to the expected test error of that model. Suppose we have a probability distribution `$p(x,y)$` and we sample from it repeatedly to generate the train set and the test set. For some fixed value `$w$`, the expected training set error is exactly the same as the expected test set error, because both expectations are formed using the same dataset sampling process. The only difference between the two conditions is the name we assign to the dataset we sample.

Of course, when we use a machine learning algorithm, we do not fix the parameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set. Under this process, the expected test error is greater than or equal to the expected value of training error. The factors determining how well a machine learning algorithm will perform are its ability to:

1. **Make the training error small.**
2. **Make the gap between training and test error small.**

These two factors correspond to the two central challenges in machine learning: **underfitting** and **overfitting**. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.

> **欠拟合(underfitting)** 是指模型不能在训练集上获得足够低的误差. 而 **过拟合 (overfitting)** 是指训练误差和和测试误差之间的差距太大.

<div align="center">
  <img src="/img_DL/ML_Basics_04_Underfitting_Overfitting.PNG" width=500px/>
</div>
<br>

We can control whether a model is more likely to overfit or underfit by altering its **capacity**. Informally, a model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set.

One way to control the capacity of a learning algorithm is by choosing its **hypothesis space**, the set of functions that the learning algorithm is allowed to select as being the solution. For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model’s capacity.

We must remember that while simpler functions are more likely to generalize (to have a small gap between training and test error) we must still choose a sufficiently complex hypothesis to achieve low training error. Typically, training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value). Typically, generalization error has a U-shaped curve as a function of model capacity.

<div align="center">
  <img src="/img_DL/ML_Basics_04_Relationship_between_Capacity_Error.PNG" width=600px/>
</div>
<br>

## The No Free Lunch Theorem

Learning theory claims that a machine learning algorithm can generalize well from a finite training set of examples. This seems to contradict some basic principles of logic. Inductive reasoning, or inferring general rules from a limited set of examples, is not logically valid. To logically infer a rule describing every member of a set, one must have information about every member of that set.

In part, machine learning avoids this problem by offering only probabilistic rules, rather than the entirely certain rules used in purely logical reasoning. Machine learning promises to find rules that are probably correct about most members of the set they concern.

Unfortunately, even this does not resolve the entire problem. The **no free lunch theorem** for machine learning (Wolpert, 1996) states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points. In other words, in some sense, no machine learning algorithm is universally any better than any other. The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class.

> 机器学习的 没有 **免费午餐定理(no free lunch theorem)** 表明 (Wolpert, 1996), 在所有可能的数据生成分布上平均之后, 每一个分类算法在未事先观测的点上都有相同的错误率. 换言之, 在某种意义上, 没有一个机器学习算法总是比其他的要好.

Fortunately, these results hold only when we average over all possible data generating distributions. If we make assumptions about the kinds of probability distributions we encounter in real-world applications, then we can design learning algorithms that perform well on these distributions.

This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the "real world" that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.

The no free lunch theorem implies that we must design our machine learning algorithms to perform well on a specific task. We do so by building a set of preferences into the learning algorithm. When these preferences are aligned with the learning problems we ask the algorithm to solve, it performs better.

We can regularize a model that learns a function f(x; θ) by adding a penalty called a **regularizer** to the cost function.

Expressing preferences for one function over another is a more general way of controlling a model’s capacity than including or excluding members from the hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.

A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies used in machine learning are explicitly designed to **reduce the test error**, possibly at the expense of increased **training error**. These strategies are known collectively as **regularization**. *Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error*. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization.

> **正则化(regularization)** 是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。

The no free lunch theorem has made it clear that there is no best machine learning algorithm, and, in particular, no best form of regularization. Instead we must choose a form of regularization that is well-suited to the particular task we want to solve.

## Parameter Norm Penalties

Many regularization approaches are based on limiting the capacity of models, such as neural networks, linear regression, or logistic regression, by adding a parameter norm penalty `$\Omega(\theta)$` to the objective function `$J$`. We denote the regularized objective function by `$\tilde{J}$`:

`$$
\tilde{J}(\theta;X,y) = J(\theta;X,y) + \alpha\Omega(\theta) \\
$$`

where `$\alpha \in [0,\infty)$` is a hyperparameter that weights the relative contribution of the norm penalty term, `$\Omega$`, relative to the standard objective function `$J$`. Setting `$\alpha$` to 0 results in no regularization. Larger values of `$\alpha$` correspond to more regularization.

When our training algorithm minimizes the regularized objective function `$\tilde{J}$` it will decrease both the original objective `$J$` on the training data and some measure of the size of the parameters `$\theta$` (or some subset of the parameters). Different choices for the parameter norm `$\Omega$` can result in different solutions being preferred.

### `$L^{2}$` Parameter Regularization

`$L^{2}$` parameter norm penalty commonly known as **weight decay**, which is one of the simplest and most common kinds of parameter norm penalty. This regularization strategy drives the weights closer to the origin1 by adding a regularization term:

 `$$
 \Omega(\theta)=\frac{1}{2}\lVert w \rVert_{2}^{2}=\frac{1}{2}\sum_{i}w_{i}^{2}
 $$`

 to the objective function. In other academic communities, L2 regularization is also known as **ridge regression** or **Tikhonov regularization**.

We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function. To simplify the presentation, we assume no bias parameter, so `$\theta$` is just `$w$`. Such a model has the following total objective function:

`$$
\tilde{J}(\theta;X,y) = \frac{\alpha}{2}w^{\mathrm{T}}w + J(\theta;X,y) \\
$$`

with the corresponding parameter gradient

`$$
\nabla_{w}\tilde{J}(\theta;X,y) = \alpha w + \nabla_{w}J(\theta;X,y) \\
$$`

To take a single gradient step to update the weights, we perform this update:

`$$
w \leftarrow w - \epsilon(\alpha w + \nabla_{w}J(\theta;X,y)) \\
$$`

Written another way, the update is:

`$$
w \leftarrow (1 - \epsilon\alpha)w  - \epsilon\nabla_{w}J(\theta;X,y) \\
$$`

We can see that the addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by a constant factor on each step, just before performing the usual gradient update.

We will further simplify the analysis by making a quadratic approximation to the objective function in the neighborhood of the value of the weights that obtains minimal unregularized training cost, `$w^{*}=\arg\max_{w}J(w)$`. If the objective function is truly quadratic, as in the case of fitting a linear regression model with mean squared error, then the approximation is perfect. The approximation `$\hat{J}$` is given by

`$$
\hat{J}(\theta) = J(w^{*}) \frac{1}{2}(w-w^{*})^{\mathrm{T}}H(w-w^{*})  \\
$$`

where `$H$` is the Hessian matrix of `$J$` with respect to `$w$` evaluated at `$w^{*}$`. The minimum of `$\hat{J}$` occurs where its gradient

`$$
\nabla_{w}\hat{J}(w) = H(w-w^{*})
$$`
is equal to 0.

To study the effect of weight decay, we could add the weight decay gradient to the equation. We can now solve for the minimum of the regularized version of `$\hat{J}$`. We use the variable `$\tilde{w}$` to represent the location of the minimum.

`$$
\begin{align*}
\alpha\tilde{w} + H(\tilde{w}-w^{*})=0 \\
(H+\alpha I)\tilde{w} = H w^{*} \\
\tilde{w}=(H+\alpha I)^{-1} H w^{*} \\
\end{align*}
$$`

As `$\alpha$` approaches 0, the regularized solution `$\tilde{w}$` approaches `$w^{*}$`. But what happens as `$\alpha$` grows? Because `$H$` is real and symmetric, we can decompose it into a diagonal matrix `$Lambda$` and an orthonormal basis of eigenvectors, `$Q$`, such that `$H=Q\Lambda Q^{\mathrm{T}}$`. Applying the decomposition to equation, we obtain:

`$$
\begin{align*}
\tilde{w}&=(Q\Lambda Q^{\mathrm{T}}+\alpha I)^{-1} Q\Lambda Q^{\mathrm{T}} w^{*} \\
&=[Q(\Lambda +\alpha I)Q^{\mathrm{T}}]^{-1} Q\Lambda Q^{\mathrm{T}} w^{*} \\
&=Q(\Lambda +\alpha I)^{-1}\Lambda Q^{\mathrm{T}} w^{*} \\
\end{align*}
$$`

We see that the effect of weight decay is to rescale `$w^{*}$` along the axes defined by the eigenvectors of `$H$`. Specifically, the component of `$w^{*}$` that is aligned with the `$i$`-th eigenvector of `$H$` is rescaled by a factor of

`$$
\frac{\lambda_{i}}{\lambda_{i}+\alpha} \\
$$`

Along the directions where the eigenvalues of `$H$` are relatively large, for example, where `$\lambda_{i} \gg \alpha$`, the effect of regularization is relatively small. However, components with `$\lambda_{i} \ll \alpha$` will be shrunk to have nearly zero magnitude.

<div align="center">
  <img src="/img_DL/03_L2_Regular.PNG" width=400px/>
</div>
<br>

Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact. In directions that do not contribute to reducing the objective function, a small eigenvalue of the Hessian tells us that movement in this direction will not significantly increase the gradient. Components of the weight vector corresponding to such unimportant directions are decayed away through the use of the regularization throughout training.

### `$L^{1}$` Parameter Regularization

While `$L^{2}$` weight decay is the most common form of weight decay, there are other ways to penalize the size of the model parameters. Another option is to use `$L^{1}$` regularization.

Formally, `$L^{1}$` regularization on the model parameter `$w$` is defined as:

`$$
\Omega(\theta) = \lVert w \rVert_{1} = \sum_{i}|w_{i}| \\
$$`

that is, as the sum of absolute values of the individual parameters. We will now discuss the effect of `$L^{1}$` regularization on the simple linear regression model, with no bias parameter, that we studied in our analysis of `$L^{2}$` regularization. As with `$L^{2}$` weight decay, `$L^{1}$` weight decay controls the strength of the regularization by scaling the penalty `$\Omega$` using a positive hyperparameter `$\alpha$`. Thus, the regularized objective function `$\tilde{J}(\theta;X,y)$` is given by

`$$
\tilde{J}(\theta;X,y) = \alpha\lVert w \rVert_{1} + J(\theta;X,y) \\
$$`

with the corresponding gradient (actually, sub-gradient):

`$$
\nabla_{w}\tilde{J}(\theta;X,y) = \alpha \mathrm{sign}(w) + \nabla_{w}J(\theta;X,y) \\
$$`

where `$\mathrm{sign}(w)$` is simply the sign of `$w$` applied element-wise.

<div align="center">
  <img src="/img_DL/03_L1_Regular.PNG" width=400px/>
</div>
<br>

> * **L0范数**：向量中非0元素的个数。
> * **L1范数(Lasso Regularization)**：向量中各个元素绝对值的和。
> * **L2范数(Ridge Regression)**：向量中各元素平方和求平方根。
>
>  L0范数和L1范数都能够达到使 **参数稀疏** 的目的，但L0范数更难优化求解，L1范数是L1的最优凸相似且更易求解，故得到广泛的应用。 因为 L1 天然的输出稀疏性，把不重要的特征都置为 0，所以它也是一个天然的**特征选择器**。L2范数主要作用是防止模型过拟合，提高模型的泛化能力。L2 计算起来更方便，而 L1 在特别是非稀疏向量上的计算效率就很低。

## Dataset Augmentation

The best way to make a machine learning model generalize better is to train it on more data. Of course, in practice, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set. For some machine learning tasks, it is reasonably straightforward to create new fake data.

Data augmentation could involve increasing the size of the available data set by augmenting them with more input created by random cropping, dilating, rotating, adding a small amount of noise, etc. The idea is to artificially create more data in the hopes that the augmented dataset will be a better representation of the underlying hidden distribution. Since we are limited by the available dataset only, this method **generally doesn’t work very well as a regularizer**.

<div align="center">
  <img src="/img_DL/03_Dataset_Augmentation.PNG" width=600px/>
</div>
<br>

##  Noise Injection

The use of noise applied to the inputs can be considered as a dataset augmentation strategy. For some models, **the addition of noise with infinitesimal variance at the input of the model** is equivalent to imposing a penalty on the norm of the weights (Bishop, 1995a,b). In the general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units.

Another way that noise has been used in the service of regularizing models is by **adding it to the weights**. This technique has been used primarily in the context of recurrent neural networks (Jim et al., 1996; Graves, 2011). This can be interpreted as a stochastic implementation of Bayesian inference over the
weights. The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.

## Early Stopping

When training large models with sufficient representational capacity to overfit the task, we often observe that training error decreases steadily over time, but validation set error begins to rise again. This behavior occurs very reliably.

This means we can obtain a model with better validation set error (and thus, hopefully better test set error) by returning to the parameter setting at the point in time with the lowest validation set error. Every time the error on the validation set improves, we store a copy of the model parameters. When the training algorithm terminates, we return these parameters, rather than the latest parameters. The algorithm terminates when no parameters have improved over the best recorded validation error for some pre-specified number of iterations.

This strategy is known as **early stopping**. It is probably the most commonly used form of regularization in deep learning. Its popularity is due both to its effectiveness and its simplicity.

Early stopping is a very unobtrusive form of regularization, in that it requires almost no change in the underlying training procedure, the objective function, or the set of allowable parameter values. This means that it is easy to use early stopping without damaging the learning dynamics. This is in contrast to weight decay, where one must be careful not to use too much weight decay and trap the network in a bad local minimum corresponding to a solution with pathologically small weights.

Early stopping may be used either alone or in conjunction with other regularization strategies. Even when using regularization strategies that modify the objective function to encourage better generalization, it is rare for the best generalization to occur at a local minimum of the training objective.

Early stopping is also useful because it reduces the computational cost of the training procedure. Besides the obvious reduction in cost due to limiting the number of training iterations, it also has the benefit of providing regularization without requiring the addition of penalty terms to the cost function or the computation of the gradients of such additional terms.

<div align="center">
  <img src="/img_DL/03_Early_Stopping.PNG" width=500px/>
</div>
<br>

**(*Left*)** The solid contour lines indicate the contours of the negative log-likelihood. The dashed line indicates the trajectory taken by SGD beginning from the origin. Rather than stopping at the point `$w^{*}$` that minimizes the cost, early stopping results in the trajectory stopping at an earlier point `$\tilde{w}$`.

**(*Right*)** An illustration of the effect of `$L^{2}$` regularization for comparison. The dashed circles indicate the contours of the `$L^{2}$` penalty, which causes the minimum of the total cost to lie nearer the origin than the minimum of the unregularized cost.

## Dropout

Dropout is used when the training model is a neural network. A neural network consists of multiple hidden layers, where the output of one layer is used as input to the subsequent layer. The subsequent layer modifies the input through learnable parameters (usually by multiplying it by a matrix and adding a bias followed by an activation function). The input flows through the neural network layers until it reaches the final output layer, which is used for prediction.

Each layer in the neural network consists of various nodes. Nodes from the previous layer are connected to nodes of the subsequent layer. In the dropout method, connections between the nodes of consecutive layers are randomly dropped based on a dropout-ratio (%age of the total connection dropped) and the remaining network is trained in the current iteration. In the next iteration, another set of random connections are dropped.

<div align="center">
  <img src="/img_DL/03_Dropout.PNG" width=600px/>
</div>
<br>

The dropout method ensures that the neural network learns a more robust set of features that perform equally well with random subsets of the node selected. By randomly dropping connections, the network is able to learn a better-generalized mapping from input to output hence reducing the over-fitting. The dropout ratio needs to be carefully selected and has a significant impact on the learned model. A good value of the dropout ratio is between 0.25 to 0.4.

## Reference

[1]  Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/regularization.html.

[2] Anwar, A. (2021, April 7). Types of regularization in machine learning. Medium. https://towardsdatascience.com/types-of-regularization-in-machine-learning-eb5ce5f9bf50.
